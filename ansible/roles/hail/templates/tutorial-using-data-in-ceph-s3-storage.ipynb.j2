{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Data in Ceph S3 Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sets up Notebook access to your S3 storage and runs through an example to check it's working.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the access key and secret key string to your bucket (supplied from IT Helpdesk when you request S3 access).\n",
    "By convention, the values used here are stored in a folder in your spark home directory: `/home/spark/.aws/credentials`.  \n",
    "<b>NOTE:</b> You'll have to update the path below if you've stored your credentials somewhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "with open(os.path.expanduser(\"~/.aws/credentials\"), 'r') as f:\n",
    "    credentials = yaml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a SparkContext object so you can use Hadoop Configuration to set your bucket keys.  \n",
    "<b>NOTE:</b> unlike the tutorial suggests, HailContext is initialised after the SparkContext and Hadoop configurations have been set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import *\n",
    "\n",
    "sparkContext = SparkContext()\n",
    "hadoopConfig = sparkContext._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoopConfig.set(\"fs.s3a.access.key\", credentials['aws_access_key_id'])\n",
    "hadoopConfig.set(\"fs.s3a.secret.key\", credentials['aws_secret_access_key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test bucket connection with tutorial data\n",
    "The tutorial data can be read from a public Sanger bucket which is read-only.  You will have to specify your bucket to demonstrate writing to a bucket.      \n",
    "<b>NOTE:</b> bucket names must be prefixed with `s3a://` so the SparkContext knows which schema to use when connecting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s3_prefix = \"s3a://\"\n",
    "sanger_bucket_name = s3_prefix + \"hail-spark-tutorial-data/\"\n",
    "your_bucket_name = s3_prefix + \"????????????\"   # Don't forget the trailing slash!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use Hail as you would normally, specifying the bucket paths as oppose to local file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hail import *\n",
    "\n",
    "hailContext = HailContext(sc=sparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### To utilise the tutorial data that is loaded into a S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial_input_file_name = \"1kg.vds\"\n",
    "\n",
    "data = hailContext.read(sanger_bucket_name + tutorial_input_file_name)\n",
    "\n",
    "data.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Read data from S3 and write the VDS to S3:\n",
    "<b>NOTE:</b> An error will occur if the output file already exists in your bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_file_name = \"example.vcf\"\n",
    "example_output_file_name = \"example.vds\"\n",
    "\n",
    "hailContext.import_vcf(sanger_bucket_name + example_input_file_name).write(your_bucket_name + example_output_file_name)\n",
    "data = hailContext.read(your_bucket_name + example_output_file_name)\n",
    "\n",
    "data.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to close your contexts when you're finished:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hailContext.stop()\n",
    "sparkContext.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Code isn't running or failing at the SparkContext/Hadoop Configuration phase\n",
    "If your code isn't running at all or you encounter an error at the SparkContext/Hadoop Configuration stage, make sure the master node is actually up and running:\n",
    "1. Open a terminal on Jupyter: Go to the Jupyter home dashboard and select 'Terminal' from the 'New' dropdown box.\n",
    "2. Go to `/usr/local/spark-{{ hail_spark_prefix_dir }}/sbin/`\n",
    "3. Run the start-master script with: `. start-master.sh`\n",
    "4. Reset the notebook kernel and try running the code again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
