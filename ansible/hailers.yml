# Copyright (c) 2017 Genome Research Ltd.
#
# Author: Joshua C. Randall <jcrandall@alum.mit.edu>
#
# This file is part of hgi-systems.
#
# hgi-systems is free software: you can redistribute it and/or modify it under
# the terms of the GNU General Public License as published by the Free Software
# Foundation; either version 3 of the License, or (at your option) any later
# version.
#
# This program is distributed in the hope that it will be useful, but WITHOUT
# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS
# FOR A PARTICULAR PURPOSE. See the GNU General Public License for more
# details.
#
# You should have received a copy of the GNU General Public License along with
# this program. If not, see <http://www.gnu.org/licenses/>.
#
---
# file: hailers.yml

- import_playbook: hgi-preamble-playbook.yml

- hosts: hailers
  vars:
    hailers_PLAYBOOK_s3_host: "{{ hgi_software_s3_credentials['s3_host'] }}"
    hailers_PLAYBOOK_s3_access_key: "{{ hgi_software_s3_credentials['s3_access_key'] }}"
    hailers_PLAYBOOK_s3_secret_key: "{{ hgi_software_s3_credentials['s3_secret_key'] }}"
    hailers_PLAYBOOK_s3_backup_bucket: "hail-backup"
    hailers_PLAYBOOK_data_backup_path: "hail/{{ hail_master_external_hostname }}.{{ hail_master_external_domain }}{{ hail_master_data_dir }}"
    hailers_PLAYBOOK_data_dir: "/mnt/{{ tf_type_openstack_compute_instance_v2_GROUP_volume_name }}"
    hailers_PLAYBOOK_created_restore_indicator_file: ".created-restore"
  tasks:
    - name: assert essential hail group membership
      assert:
        that: 
          - "inventory_hostname in groups['hail-masters'] or
             inventory_hostname in groups['hail-computers']"
        msg: "all hosts in hailers group must also be in group hail-masters and/or hail-computers"
    - name: assert required vars defined in hail-cluster-<hail_cluster_id>
      assert:
        that: 
          - "hail_cluster_id is defined"
          - "hail_cluster_jupyter_token is defined"
          - "hail_cluster_spark_build is defined"
          - "hail_cluster_hadoop_version is defined"
          - "hail_cluster_ssh_key is defined"
        msg: "'hail_cluster_id', 'hail_cluster_jupyter_token', 'hail_cluster_spark_build', 'hail_cluster_hadoop_version', and 'hail_cluster_ssh_key' must all be defined (probably in group hail-cluster-{% if hail_cluster_id is defined %}{{ hail_cluster_id }}{% else %}<hail_cluster_id>{% endif %})"
    - name: assert hail_cluster_id must be a string
      assert:
        that:
          - "hail_cluster_id is string"
        msg: "hail_cluster_id must be a string (it is {{ hail_cluster_id }}) - did you forget to quote a numeric value?"
    - name: assert required vars defined in hgi-credentials
      assert:
        that: 
          - "hgi_credentials_pem_WILDCARD_hgi_sanger_ac_uk_cert is defined"
          - "hgi_credentials_pem_WILDCARD_hgi_sanger_ac_uk_key is defined"
        msg: "'hgi_credentials_pem_WILDCARD_hgi_sanger_ac_uk_cert' and 'hgi_credentials_pem_WILDCARD_hgi_sanger_ac_uk_key' must all be defined (probably in group hgi-credentials)"
    - name: assert required vars defined in hgi
      assert:
        that: 
          - "hgi_software_s3_bucket is defined"
          - "hgi_software_s3_credentials is defined"
        msg: "'hgi_software_s3_bucket' and 'hgi_software_s3_credentials' must be defined (probably in group hgi)"
    - name: assert s3 credentials are available
      assert:
        that: 
          - "'s3_host' in hgi_software_s3_credentials"
          - "'s3_access_key' in hgi_software_s3_credentials"
          - "'s3_secret_key' in hgi_software_s3_credentials"
        msg: "'s3_host', 's3_access_key', and 's3_secret_key' must all be set in hgi_software_s3_credentials (is host a member of s3-credentials group?)"
    - name: assert infoblox record is available
      assert:
        that:
          - "'tf.'+terraform_tenant+'.infoblox_record.hail-'+hail_cluster_id in hostvars"
        msg: "infoblox (non-host) inventory entry not found for {{ 'tf.'+terraform_tenant+'.infoblox_record.hail-'+hail_cluster_id }}"
    - name: assert that infoblox record has tf_name and tf_domain hostvars
      assert:
        that:
          - "'tf_name' in hostvars['tf.'+terraform_tenant+'.infoblox_record.hail-'+hail_cluster_id]"
          - "'tf_domain' in hostvars['tf.'+terraform_tenant+'.infoblox_record.hail-'+hail_cluster_id]"
        msg: "'tf_name' and 'tf_domain' are both required hostvars for {{ 'tf.'+terraform_tenant+'.infoblox_record.hail-'+hail_cluster_id }}"
    - name: assert terraform volume variables are set for hail master
      assert:
        that:
          - "tf_type_openstack_compute_instance_v2_GROUP_volume_name != ''"
          - "tf_type_openstack_compute_instance_v2_GROUP_volume_size != ''"
          - "tf_type_openstack_compute_instance_v2_GROUP_volume_device != ''"
      when: inventory_hostname in groups['hail-masters']
    - name: configure attached openstack volume for hail master
      import_role: 
        name: attached-openstack-volume
      vars:
        attached_openstack_volume_vg: hail-vg
        attached_openstack_volume_vols:
          - name: hail-data
            size: "{{ ((tf_type_openstack_compute_instance_v2_GROUP_volume_size | int) * 1024) - 4 }}M"
            fstype: xfs
            mountpoint: "{{ hailers_PLAYBOOK_data_dir }}"
        attached_openstack_volumes:
          - device: "{{ tf_type_openstack_compute_instance_v2_GROUP_volume_device }}"
        attached_openstack_volume_fs_create_indicator_file: "{{ hailers_PLAYBOOK_created_restore_indicator_file }}"
      when: inventory_hostname in groups['hail-masters']
    - name: get spark url and checksum from s3
      tags: s3
      import_role:
        name: s3_geturl
      vars:
        s3_geturl_bucket: hgi-software
        s3_geturl_url: "https://{{ hailers_PLAYBOOK_s3_host }}"
        s3_geturl_access_key: "{{ hailers_PLAYBOOK_s3_access_key }}"
        s3_geturl_secret_key: "{{ hailers_PLAYBOOK_s3_secret_key }}"
        s3_geturl_object: "spark-{{ hail_cluster_spark_build }}.tgz"
        s3_geturl_checksum_var: "hailers_PLAYBOOK_spark_tgz_checksum"
        s3_geturl_url_var: "hailers_PLAYBOOK_spark_tgz_url"
    - name: get hadoop url and checksum from s3
      tags: s3
      import_role:
        name: s3_geturl
      vars:
        s3_geturl_bucket: hgi-software
        s3_geturl_url: "https://{{ hailers_PLAYBOOK_s3_host }}"
        s3_geturl_access_key: "{{ hailers_PLAYBOOK_s3_access_key }}"
        s3_geturl_secret_key: "{{ hailers_PLAYBOOK_s3_secret_key }}"
        s3_geturl_object: "hadoop-{{ hail_cluster_hadoop_version }}.tgz"
        s3_geturl_checksum_var: "hailers_PLAYBOOK_hadoop_tgz_checksum"
        s3_geturl_url_var: "hailers_PLAYBOOK_hadoop_tgz_url"
    # TODO: Create and populate hail-tutorial-data S3 bucket.
    - name: import role hail
      import_role:
        name: hail
      vars:
        ###############################################################################
        # Variables that have no defaults and must be set
        ###############################################################################
        hail_jupyter_token: "{{ hail_cluster_jupyter_token }}"
        hail_ssh_key: "{{ hail_cluster_ssh_key }}"
        hail_ssl_cert: "{{ hgi_credentials_pem_WILDCARD_hgi_sanger_ac_uk_cert }}"
        hail_ssl_key: "{{ hgi_credentials_pem_WILDCARD_hgi_sanger_ac_uk_key }}"
        hail_master_host_list: "{{ (groups['hail-masters'] | map('extract', hostvars) | selectattr('hail_cluster_id', 'equalto', hail_cluster_id)) | map(attribute='inventory_hostname') | list() }}"
        hail_computer_host_list: "{{ (groups['hail-computers'] | map('extract', hostvars) | selectattr('hail_cluster_id', 'equalto', hail_cluster_id)) | map(attribute='inventory_hostname') | list() }}"
        hail_master_data_dir: "{{ hailers_PLAYBOOK_data_dir }}"
        hail_master_external_hostname: "{{ hostvars['tf.'+terraform_tenant+'.infoblox_record.hail-'+hail_cluster_id]['tf_name'] }}"
        hail_master_external_domain: "{{ hostvars['tf.'+terraform_tenant+'.infoblox_record.hail-'+hail_cluster_id]['tf_domain'] }}"
        hail_backup_s3_host: "{{ hailers_PLAYBOOK_s3_host }}"
        hail_backup_s3_access_key: "{{ hailers_PLAYBOOK_s3_access_key }}"
        hail_backup_s3_secret_key: "{{ hailers_PLAYBOOK_s3_secret_key }}"
        hail_tutorial_data_s3_bucket: "hail-tutorial-data"
        ###############################################################################
        # Versions
        ###############################################################################
        hail_version: "{{ hail_cluster_hail_version | default('0.1-0bd1988e') }}"
        #hail_version_branch: "{{ hail_version | regex_replace('(.*)[-](.*)', '\\1') }}"
        #hail_version_commit: "{{ hail_version | regex_replace('(.*)[-](.*)', '\\2') }}"
        hail_spark_build_version: "{{ hail_cluster_spark_build }}"
        #hail_spark_version: "{{ hail_spark_build_version | regex_replace('([0-9.]+)-.*', '\\1') }}"
        hail_spark_hadoop_version: "{{ hail_cluster_hadoop_version }}"
        hail_seaborn_version: "{{ hail_cluster_seaborn_version | default('0.8.0') }}"
        hail_anaconda_version: "{{ hail_cluster_anaconda_version | default('4.4.0') }}"
        hail_python_version: "{{ hail_cluster_python_version | default('2') }}"
        ###############################################################################
        # Directories
        ###############################################################################
        #hail_temp_dir: /tmp/hail
        #hail_prefix_dir: "/usr/local/hail-{{ hail_version }}"
        #hail_spark_prefix_dir: "/usr/local/spark-{{ hail_spark_build_version }}"
        #hail_systemd_service_dir: /etc/systemd/system
        #ail_anaconda_prefix_dir: "/usr/local/anaconda-{{ hail_anaconda_version }}"
        #hail_ssl_key_file: /etc/ssl.key
        #hail_ssl_cert_file: /etc/ssl.cert
        ###############################################################################
        # Users/Groups
        ###############################################################################
        hail_user: "{{ hail_cluster_user | default('hail') }}"
        hail_group: "{{ hail_cluster_group | default(hail_user) }}"
        hail_authorized_keys: "{{ hail_cluster_authorized_keys | default([]) }}"
        ###############################################################################
        # Source repo
        ###############################################################################
        #hail_repository: https://github.com/hail-is/hail.git
        #hail_check_installed: "{{ hail_prefix_dir }}/jars/hail-all-spark.jar"
        ###############################################################################
        # Hail Jupyter service
        ###############################################################################
        #hail_jupyter_bin_file: "{{ hail_prefix_dir }}/scripts/jhail"
        #hail_jupyter_config_file: "/etc/opt/jupyter_config_file.py"
        #hail_jupyter_server_name: "{{ hail_master_external_hostname }}.{{ hail_master_external_domain }}"
        #hail_jupyter_backend_port: 8888
        #hail_service_start: "{{ hail_jupyter_bin_file }} --config={{ hail_jupyter_config_file }}"
        #hail_service_user: "{{ hail_user }}"
        #hail_service_initial_path: /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
        #hail_service_environment_variables:
        #  PATH: "{{ hail_service_initial_path }}:{{ hail_anaconda_prefix_dir }}/bin:{{ hail_prefix_dir }}/bin"
        #  HAIL_HOME: "{{ hail_prefix_dir }}"
        #  SPARK_HOME: "{{ hail_prefix_dir }}"
        #  PYTHONPATH: "{{ hail_prefix_dir }}/python:{{ hail_prefix_dir }}/python:{{ hail_prefix_dir }}/python/lib/py4j-0.10.4-src.zip"
        ###############################################################################
        # Backup settings
        ###############################################################################
        hail_backup_s3_backup_bucket: "{{ hailers_PLAYBOOK_s3_backup_bucket }}"
        hail_data_backup_path: "{{ hailers_PLAYBOOK_data_backup_path }}"
        hail_s3_backup_restore_indicator_file: "{{ hailers_PLAYBOOK_created_restore_indicator_file }}"
        ###############################################################################
        # Spark settings
        ###############################################################################
        hail_spark_tgz_url: "{{ hailers_PLAYBOOK_spark_tgz_url }}"
        hail_spark_tgz_checksum: "{{ hailers_PLAYBOOK_spark_tgz_checksum }}"
        hail_spark_hadoop_tgz_url: "{{ hailers_PLAYBOOK_hadoop_tgz_url }}"
        hail_spark_hadoop_tgz_checksum: "{{ hailers_PLAYBOOK_hadoop_tgz_checksum }}"
        #hail_spark_master_backend_port: "8080"
        hail_spark_local_ip: "{{ cloud_resource_private_ip }}"
        hail_spark_hadoop_anonymous_buckets:
         - "{{ hail_tutorial_data_s3_bucket }}"
        hail_master_host: "{{ hostvars[hail_master_host_list[0]]['cloud_resource_private_ip'] }}"
        hail_spark_executor_memory_gb: 60 # FIXME make this depend on node flavour (in this case 64GB node -> 60g executor memory)
        ###############################################################################
        # General settings
        ###############################################################################
        hail_apt_cache_valid_time: "{{ all_apt_cache_valid_time }}"
      tags:
        - hail
